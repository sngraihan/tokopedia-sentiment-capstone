{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WexU_OzffE3n"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 0) Library\n",
        "# ================================\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import joblib\n",
        "import os, json, time\n",
        "\n",
        "# Repro\n",
        "RSEED = 42\n",
        "np.random.seed(RSEED)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 1) Load data bersih\n",
        "# ================================\n",
        "df = pd.read_csv(\"tokopedia_reviews_clean.csv\")\n",
        "\n",
        "# Pastikan kolom tersedia\n",
        "assert set([\"text\",\"sentiment\"]).issubset(df.columns), \"Kolom 'text'/'sentiment' tidak ditemukan.\"\n",
        "\n",
        "# Drop NA/duplikat minimal\n",
        "df = df.dropna(subset=[\"text\",\"sentiment\"]).drop_duplicates(subset=[\"text\"]).reset_index(drop=True)\n",
        "print(df.head(3))\n",
        "print(df[\"sentiment\"].value_counts())\n"
      ],
      "metadata": {
        "id": "JhnLYGNAfnOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 2) Train/Val/Test split (stratified)\n",
        "# ================================\n",
        "X = df[\"text\"].astype(str).values\n",
        "y = df[\"sentiment\"].astype(str).values\n",
        "\n",
        "# train 70%, val 15%, test 15%\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y, test_size=0.15, random_state=RSEED, stratify=y\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.1765, random_state=RSEED, stratify=y_temp\n",
        ")  # 0.1765 * 0.85 â‰ˆ 0.15\n",
        "\n",
        "print(f\"Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dx2eX5kTfl_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 3) Pipeline TF-IDF + Logistic Regression\n",
        "#    - class_weight='balanced' biar imbang antar kelas\n",
        "#    - sublinear_tf=True buat handle kata frekuensi tinggi\n",
        "# ================================\n",
        "pipe = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(\n",
        "        ngram_range=(1,2),\n",
        "        min_df=2,\n",
        "        max_df=0.95,\n",
        "        sublinear_tf=True\n",
        "    )),\n",
        "    (\"clf\", LogisticRegression(\n",
        "        max_iter=2000,\n",
        "        class_weight=\"balanced\",\n",
        "        n_jobs=None # ignored by LR liblinear/saga; biarin default\n",
        "    ))\n",
        "])"
      ],
      "metadata": {
        "id": "HooPr88XfkKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 4) GridSearch (hiperparameter sederhana)\n",
        "# ================================\n",
        "param_grid = {\n",
        "    \"clf__C\": [0.5, 1.0, 2.0, 4.0],\n",
        "    \"clf__solver\": [\"lbfgs\", \"saga\"],  # saga support l1/l2, lbfgs l2\n",
        "    # tambahkan l1 penalty opsional\n",
        "    # \"clf__penalty\": [\"l2\"]  # kalau mau l1, tambahkan \"l1\" dan solver=\"saga\"\n",
        "}\n",
        "gs = GridSearchCV(\n",
        "    pipe,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"f1_macro\",\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "gs.fit(X_train, y_train)\n",
        "print(\"Best params:\", gs.best_params_)\n",
        "print(\"Best CV score (macro-F1):\", gs.best_score_)"
      ],
      "metadata": {
        "id": "wMOHQki_fi5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 5) Evaluasi di VAL set\n",
        "# ================================\n",
        "best_model = gs.best_estimator_\n",
        "\n",
        "y_val_pred = best_model.predict(X_val)\n",
        "print(\"\\n=== VAL SET REPORT ===\")\n",
        "print(classification_report(y_val, y_val_pred, digits=4))\n",
        "\n",
        "cm_val = confusion_matrix(y_val, y_val_pred, labels=[\"neg\",\"neu\",\"pos\"])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_val, display_labels=[\"neg\",\"neu\",\"pos\"])\n",
        "plt.figure()\n",
        "disp.plot(values_format=\"d\")\n",
        "plt.title(\"Confusion Matrix - Validation\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9-iv9Tv-fhv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 6) Evaluasi FINAL di TEST set\n",
        "# ================================\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "print(\"\\n=== TEST SET REPORT ===\")\n",
        "print(classification_report(y_test, y_test_pred, digits=4))\n",
        "\n",
        "cm_test = confusion_matrix(y_test, y_test_pred, labels=[\"neg\",\"neu\",\"pos\"])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=[\"neg\",\"neu\",\"pos\"])\n",
        "plt.figure()\n",
        "disp.plot(values_format=\"d\")\n",
        "plt.title(\"Confusion Matrix - Test\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GRXTXN8DfgV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 7) Simpan artefak model (pipeline) + meta\n",
        "# ================================\n",
        "os.makedirs(\"artefacts\", exist_ok=True)\n",
        "stamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "model_path = f\"artefacts/sentiment_lr_pipeline_{stamp}.joblib\"\n",
        "joblib.dump(best_model, model_path)\n",
        "\n",
        "meta = {\n",
        "    \"created_at\": stamp,\n",
        "    \"model_path\": model_path,\n",
        "    \"algo\": \"TFIDF+LogReg\",\n",
        "    \"params\": gs.best_params_,\n",
        "    \"classes\": [\"neg\",\"neu\",\"pos\"],\n",
        "    \"notes\": \"Baseline sentiment Tokopedia\"\n",
        "}\n",
        "with open(f\"artefacts/metadata_{stamp}.json\",\"w\") as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "\n",
        "print(\"Saved:\", model_path)"
      ],
      "metadata": {
        "id": "KnrWg0TdffGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 8) Quick inference helper\n",
        "# ================================\n",
        "def predict_sentiment(texts):\n",
        "    \"\"\"texts: list[str]\"\"\"\n",
        "    loaded = joblib.load(model_path)\n",
        "    preds = loaded.predict(texts)\n",
        "    proba = None\n",
        "    # cek apakah model punya predict_proba\n",
        "    try:\n",
        "        proba = loaded.predict_proba(texts)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return preds, proba\n",
        "\n",
        "# Contoh pakai:\n",
        "samples = [\n",
        "    \"Barangnya bagus, pengiriman cepat banget. Recommended!\",\n",
        "    \"Biasa aja sih. Tidak terlalu sesuai ekspektasi.\",\n",
        "    \"Parah, barang rusak dan penjual susah dihubungi.\"\n",
        "]\n",
        "preds, prob = predict_sentiment(samples)\n",
        "for i, s in enumerate(samples):\n",
        "    print(f\"[{preds[i]}] {s}\")"
      ],
      "metadata": {
        "id": "M6iORWNKfd6E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}